In paper Section 6 (Performance Evaluation), Figure 5 presents the performance results of Sulfur on the Embench-IoT benchmark suite. 
Each data point represents the average of 10 runs, and the overall overhead is reported as the geometric mean of per-benchmark slowdown 
ratios. The results show that Sulfur on Linux introduces only ~1.67% overhead relative to the baseline, indicating a negligible impact on 
execution time.By contrast, CFA incurs substantially higher overheads,with ~59.88% on Linux and ~62.99% on Sulfur.Furthermore,CFA-Sulfur
demonstrates only a modest additional slowdown of ~1.94% compared to CFA-Linux. These findings clearly suggest that the performance 
degradation is primarily attributed to CFA, while Sulfur itself imposes virtually no measurable overhead.

In paper Section 6 (Performance Evaluation), Figure 7 reports the overhead incurred by Sulfur on system operations,as measured using 
LMBench micro-benchmarks. The results indicate substantial increases in system call and kernel operation latencies compared to Linux. For 
instance, the null system call shows the highest overhead at ~452.81%, while operations such as read (~281.94%), write (~308.40%), and 
signal handler installation (~259.21%) also experience significant slowdowns. Moderate overheads are observed for stat (~58.81%), 
open/close (~76.77%), and signal dispatch (~56.52%), whereas page faults incur the lowest overhead at ~35.50%. Context switching overhead 
is ~108.58%, reflecting the additional scheduling complexity introduced by Sulfur. Overall,these results show that while Sulfur maintains 
functional correctness, it introduces considerable latency in low-level system operations due to its additional safety and monitoring 
mechanisms.

We provide a build.sh script for building the FVP machine with the appropriate configuration (either baseline or Sulfur). For the baseline configuration, benchmarks can be executed using build.sh baseline, and for Sulfur, the argument sulfur should be passed instead (see Step 4 of the Steps to Build and Run section in the README).

After building, you will obtain the FVP terminals. In the FVP environment, you need to invoke the script located at /usr/bin/run.sh with the corresponding argument. The run.sh script included in the claim/ directory is copied into the FVP machineâ€™s filesystem during setup and can then be used from /usr/bin/run.sh within the FVP machine. This will run the two benchmarks, lmbench and embench, under the selected configuration mode (see Step 5 of the Steps to Build and Run section in the README).

It is important to note that the results provided in the expected/ directory were obtained in the FVP environment, whereas the results
presented in the paper were measured on a Raspberry Pi 3 B+ platform. Therefore, the performance numbers produced in FVP are not expected 
to exactly match those reported in the paper. To reproduce the precise results from the paper, the implementation must be ported to the 
Raspberry Pi 3 B+. We are happy to provide guidance and support should reviewers wish to conduct experiments on Raspberry Pi.
